# Awesome-Visual-Captioning[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
  <img width="250" src="https://camo.githubusercontent.com/1131548cf666e1150ebd2a52f44776d539f06324/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f6d61737465722f6d656469612f6c6f676f2e737667" "Awesome!">
</p>

A curated list of image captioning and video captioning and related area. :-)
***
This repository focus on Image Captioning & Video Captioning &amp; Seq-to-Seq Learning &amp; NLP

## Newly update! CVPR-2019
**Image Captioning**  
- :XU YANG (Nanyang Technological University)*; Kaihua Tang (Nanyang Technological University); Hanwang Zhang (Nanyang Technological University); Jianfei Cai (Nanyang Technological University)  
"[Auto-Encoding Scene Graphs for Descriptive Image Captioning](https://arxiv.org/abs/1812.02378)". `CVPR 2019 Oral` [[code]](https://github.com/fengyang0317/unsupervised_captioning)

- :Aditya Deshpande (University of Illinois at UC)*; Jyoti Aneja (University of Illinois, Urbana-Champaign); Liwei Wang (Tencent AI Lab); Alexander Schwing (UIUC); David Forsyth (Univeristy of Illinois at Urbana-Champaign)  
"[Fast, Diverse and Accurate Image Captioning Guided By Part-of-Speech](https://arxiv.org/pdf/1805.12589v2.pdf)". `CVPR 2019 Oral`

- :Yang Feng (University of Rochester)*; Lin Ma (Tencent AI Lab); Wei Liu (Tencent); Jiebo Luo (U. Rochester)  
"[Unsupervised Image Captioning](https://arxiv.org/pdf/1811.10787.pdf)". `CVPR 2019`

- :Yan Xu (UESTC); Baoyuan Wu (Tencent AI Lab)*; Fumin Shen (UESTC); Yanbo Fan (Tencent AI Lab); Yong Zhang (Tencent AI Lab); Heng Tao Shen (University of Electronic Science and Technology of China (UESTC)); Wei Liu (Tencent)  
"Adversarial Attack to Image Captioning via Structured Output Learning with Latent Variables". `CVPR 2019` 

- :Qingzhong Wang (Department of Computer Science, City University of Hong Kong)*; Antoni Chan (City University of Hong Kong, Hong, Kong)  
"[Describing like Humans: on Diversity in Image Captioning](https://arxiv.org/pdf/1903.12020.pdf)". `CVPR 2019` 

- :Longteng Guo ( Institute of Automation, Chinese Academy of Sciences)*; Jing Liu (National Lab of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences); Peng Yao (University of Science and Technology Beijing); Jiangwei Li (Huawei); Hanqing Lu (NLPR, Institute of Automation, CAS)  
"MSCap: Multi-Style Image Captioning with Unpaired Stylized Text". `CVPR 2019`

- :Lu Zhang (Dalian University of Technology); Huchuan Lu (Dalian University of Technology)*; Zhe Lin (Adobe Research); Jianming Zhang (Adobe Research); You He (Naval Aviation University)  
"[CapSal: Leveraging Captioning to Boost Semantics for Salient Object Detection](https://pan.baidu.com/s/1hS38zj_xK9f9KBtY_hkWzQ)". `CVPR 2019` [[code]](https://github.com/zhangludl/code-and-dataset-for-CapSal)

- :Guojun Yin (University of Science and Technology of China); Lu Sheng (The Chinese University of Hong Kong)*; Bin Liu (University of Science and Technology of China); Nenghai Yu (University of Science and Technology of China); Xiaogang Wang (Chinese University of Hong Kong, Hong Kong); Jing Shao (Sensetime)  
"[Context and Attribute Grounded Dense Captioning](https://arxiv.org/pdf/1904.01410.pdf)". `CVPR 2019`

- :Dong-Jin Kim (KAIST)*; Jinsoo Choi (KAIST); Tae-Hyun Oh (MIT CSAIL); In So Kweon (KAIST)  
"[Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning](https://arxiv.org/pdf/1903.05942.pdf)". `CVPR 2019`

- :Marcella Cornia (University of Modena and Reggio Emilia); Lorenzo Baraldi (University of Modena and Reggio Emilia)*; Rita Cucchiara (Universita Di Modena E Reggio Emilia)  
"[Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions](https://arxiv.org/pdf/1811.10652.pdf)".. `CVPR 2019`

- :Junlong Gao (Peking University Shenzhen Graduate School)*; Shiqi Wang (CityU); Shanshe Wang (Peking University); Siwei Ma (Peking University, China); Wen Gao (PKU)  
"[Self-critical n-step Training for Image Captioning](https://arxiv.org/pdf/1904.06861.pdf)". `CVPR 2019`

- :Yu Qin (Shanghai Jiao Tong University)*; Jiajun Du (Shanghai Jiao Tong University); Hongtao Lu (Shanghai Jiao Tong University); Yonghua Zhang (Bytedance)  
"Look Back and Predict Forward in Image Captioning". `CVPR 2019`

- :Yue Zheng (Tsinghua University); Ya-Li Li (THU); Shengjin Wang (Tsinghua University)*  
"[Intention Oriented Image Captions with Guiding Objects](https://arxiv.org/pdf/1811.07662.pdf)". `CVPR 2019`

- :Pierre Dognin (IBM)*; Igor Melnyk (IBM); Youssef Mroueh (IBM Research); Jarret Ross (IBM); Tom Sercu (IBM Research AI)  
"[Adversarial Semantic Alignment for Improved Image Captions](https://arxiv.org/pdf/1805.00063.pdf)". `CVPR 2019`

- :Ali Furkan Biten (Computer Vision Center)*; Lluis Gomez (Universitat Autónoma de Barcelona); Marçal Rusiñol (Computer Vision Center, UAB); Dimosthenis Karatzas (Computer Vision Centre)  
"[Good News, Everyone! Context driven entity-aware captioning for news images](https://arxiv.org/pdf/1904.01475.pdf)". `CVPR 2019`

- :Yehao Li (Sun Yat-Sen University); Ting Yao (JD AI Research); Yingwei Pan (JD AI Research)*; Hongyang Chao (Sun Yat-sen University); Tao Mei (AI Research of JD.com)  
"[Pointing Novel Objects in Image Captioning](https://arxiv.org/pdf/1904.11251.pdf)". `CVPR 2019`

- :Kurt Shuster (Facebook)*; Samuel Humeau (Facebook); Hexiang Hu (USC); Antoine Bordes (Facebook); Jason Weston (FAIR)  
"[Engaging Image Captioning via Personality](https://arxiv.org/pdf/1810.10665.pdf)". `CVPR 2019`

**Video Captioning**
- :Jonghwan Mun (POSTECH)*; Linjie Yang (ByteDance AI Lab); Zhou Ren (Snap Inc.); Ning Xu (Snap); Bohyung Han (Seoul National University)  
"[Streamlined Dense Video Captioning](https://arxiv.org/pdf/1904.03870.pdf)". `CVPR 2019 Oral`

- :Dahun Kim (KAIST)*; Sanghyun Woo (KAIST); Joon-Young Lee (Adobe Research); In So Kweon (KAIST)  
"[Deep Blind Video Decaptioning by Temporal Aggregation and Recurrence](https://arxiv.org/pdf/1905.02949v1.pdf)". `CVPR 2019`

- :Junchao Zhang (Peking University); Yuxin Peng (Peking University)*  
"Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning". `CVPR 2019`

- :Wenjie Pei (Tencent)*; Jiyuan Zhang (Tencent YouTu); Xiangrong Wang (Delft University of Technology); Lei Ke (Tencent); Xiaoyong Shen (Tencent); Yu-Wing Tai (Tencent)  
"Memory-Attended Recurrent Network for Video Captioning". `CVPR 2019`

- :Nayyer Aafaq (The University of Western Australia)*; Naveed Akhtar (The University of Western Australia); Wei Liu (University of Western Australia); Syed Zulqarnain Gilani (The University of Western Australia); Ajmal Mian (University of Western Australia)  
"[Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning](https://arxiv.org/pdf/1902.10322v1.pdf)". `CVPR 2019`

- :Luowei Zhou (University of Michigan)*; Yannis Kalantidis (Facebook Research); Xinlei Chen (Facebook AI Research); Jason J Corso (University of Michigan); Marcus Rohrbach (Facebook AI Research)  
"[Grounded Video Description](https://arxiv.org/pdf/1812.06587)". `CVPR 2019`
  
- :Jae Sung Park (UC Berkeley); Marcus Rohrbach (Facebook AI Research); Trevor Darrell (UC Berkeley); Anna Rohrbach (UC Berkeley)*	
"[Adversarial Inference for Multi-Sentence Video Description](https://arxiv.org/pdf/1812.05634.pdf)". `CVPR 2019`

## Newly update! AAAI-2019
**Image Captioning**  
- 5123:CHEN CHEN (Tencent)*; SHUAI MU (Tencent); WANPENG XIAO (Tencent); ZEXIONG YE (Tencent); LIESI WU (Tencent); QI JU (Tencent)   
"[Improving Image Captioning with Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1805.07112.pdf)". `AAAI 2019 Oral`

- 3934:Lingyun Song (Xi'an JiaoTong University)*; Jun Liu (Xi'an Jiaotong Univerisity); Buyue Qian (Xi'an Jiaotong University); Yihe Chen (University of Toronto)  
"Connecting Language to Images: A Progressive Attention-Guided Network for Simultaneous Image Captioning and Language Grounding". `AAAI 2019 Oral`

- 4938:Nannan Li (Wuhan University); Zhenzhong Chen (WHU)*; Shan Liu (Tencent America)  
"Meta Learning for Image Captioning". `AAAI 2019`

- 5390:Lianli Gao (The University of Electronic Science and Technology of China); kaixuan fan (University of Electronic Science and Technology of China); Jingkuan Song (UESTC); Xianglong Liu (Beihang University); Xing Xu (University of Electronic Science and Technology of China); Heng Tao Shen (University of Electronic Science and Technology of China (UESTC))*  
"[Deliberate Residual based Attention Network for Image Captioning](https://www.aaai.org/Papers/AAAI/2019/AAAI-GaoLianli3.5390.pdf)". `AAAI 2019`

- 1410:Weixuan Wang (School of Electronic and Information Engineering, Sun Yat-sen University); Zhihong Chen (School of Electronic and Information Engineering, Sun Yat-sen University); Haifeng Hu (School of Electronic and Information Engineering, Sun Yat-sen University)* 
"Hierarchical Attention Network for Image Captioning". `AAAI 2019`


**Video Captioning**  
- 1469:Xin Wang (University of California, Santa Barbara)*; Jiawei Wu (University of California, Santa Barbara); Da Zhang (UC Santa Barbara); Yu Su (OSU); William Wang (UC Santa Barbara)  
"[Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning](https://arxiv.org/pdf/1811.02765.pdf)". `AAAI 2019 Oral`

- 2277:Jingwen Chen (Sun Yat-set University); Yingwei Pan (JD AI Research)*; Yehao Li (Sun Yat-Sen University); Ting Yao (JD AI Research); Hongyang Chao (Sun Yat-sen University); Tao Mei (AI Research of JD.com)  
"[Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning](home.ustc.edu.cn/~panywei/paper/AAAI19.2277.pdf)". `AAAI 2019 Oral`

- 2389:Kuncheng Fang (Fudan University)*; Lian Zhou (Fudan University); Cheng Jin (Fudan University); Yuejie Zhang (Fudan University); Kangnian Weng (Shanghai University of Finance and Economics); Tao Zhang (Shanghai University of Finance and Economics); Weiguo Fan (University of Iowa) 
"Fully Convolutional Video Captioning with Coarse-to-Fine and Inherited Attention". `AAAI 2019`

- 2732:Shaoxiang Chen (Fudan University)*; Yu-Gang Jiang (Fudan University)  
"[Motion Guided Spatial Attention for Video Captioning](http://yugangjiang.info/publication/19AAAI-vidcaptioning.pdf)". `AAAI 2019`

- 3468:Xiangyang Li (Institute of Computing Technology, Chinese Academy of Sciences); Shuqiang Jiang (ICT, China Academy of Science)*; Jungong Han (Lancaster University)  
"Learning Object Context for Dense Captioning". `AAAI 2019` 





<details>
<summary>Personal summary</summary>
  
## Image Captioning

### Model
- Unsupervised Image Captioning.(**Yang Feng**, Tencent AI Lab, CVPR2019, [[paper]](https://arxiv.org/pdf/1811.10787.pdf), [[code.tf]](https://github.com/fengyang0317/unsupervised_captioning))
- Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions.(*state-of-the-art*, **Cornia Marcella**, CVPR2019, [[paper]](https://arxiv.org/pdf/1811.10652v2.pdf), [[code.pytorch]](https://github.com/aimagelab/show-control-and-tell))  
- Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning.(visual sentinel + adaptive attention, **Jiasen Lu**, CVPR2017)  
- Recurrent Fusion Network for Image Captioning.(fusion muti-CNNs' feature as encoder, **Wenhao Jiang**, Tencent AI Lab, ECCV2018)
- Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space.(**Liwei Wang**, NIPS2017, condition-VAE for caption, [[paper]](https://arxiv.org/pdf/1711.07068.pdf))  
- Rethinking the Form of Latent States in Image Captioning. (ConvLSTM, **DaiBo**, ECCV2018)
- Discriminability objective for training descriptive captions.(retrival+caption, **RuoTian Luo**, CVPR2018)
- Exploring Visual Relationship for Image Captioning.(Detection + GCN, **Ting Yao**, ECCV2018)
- Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data.(ECCV2018)
- “Factual” or “emotional”: Stylized image captioning with adaptive learning and attention.(ECCV2018)
- Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering.(**Peter Anderson**, CVPR2018)
- Neural Baby Talk.(*Pointer Network*, **Jiasen Lu**, CVPR2018)
- Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning.(**Jiasen Lu**, CVPR2017)
- Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.(ICML2015)

### Training Strategy

- Self-critical sequence training for image captioning.(CVPR2017)
- Sequence-to-Sequence Learning as Beam-Search Optimization

### Evaluation

- Improving Image Captioning with Conditional Generative Adversarial Nets.(AAAI2019, [[paper]](https://arxiv.org/pdf/1805.07112.pdf))
- NNEval : Neural Network based Evaluation Metric for Image Captioning.(ECCV2018)
- Improved Image Captioning via Policy Gradient optimization of SPIDEr.(ICCV2017)

## NLP

- Attention is all you need([The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html), [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/), [Transformer on ImgCaption](https://github.com/ruotianluo/Transformer_Captioning/blob/master/train.py))
- Latent Alignment and Variational Attention(VAE, [[paper]](https://arxiv.org/pdf/1807.03756.pdf))
- Variational Neural Machine Translation(VAE for NMT, [[paper]](https://arxiv.org/pdf/1605.07869.pdf))
- Variational Recurrent Neural Machine Translation(VAE for NMT, [[paper]](https://arxiv.org/pdf/1801.05119.pdf))

## Video Captioning

- Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning(AAAI2019 **oral**, [[paper]](http://home.ustc.edu.cn/~panywei/paper/AAAI19.2277.pdf))
- Less Is More: Picking Informative Frames for Video Captioning(ECCV2018)
- Modeling Embedding and Translation to Bridge Video and Language(CVPR2016)
- Video Captioning via Hierarchical Reinforcement Learning(CVPR2018)
- End-to-End Dense Video Captioning with Masked Transformer(CVPR2018)

## Arxiv Update

- Viewpoint Invariant Change Captioning
- Not All Words are Equal : Video-specific Information Loss for Video Captioning(Jiarong Dong)
- Hierarchical LSTMs with Adaptive Attention for Visual Captioning

## New Idea

- Pointer Network. 
- VideoLSTM Convolves, Attends and Flows for Action Recognition. (ConvLSTM, [[paper]](http://export.arxiv.org/pdf/1607.01794))
- Detecting and Recognizing Human-Object Interactions(**Ross Girshick**, **Piotr Dollar**, **Kaiming He**, [[paper]](https://arxiv.org/pdf/1704.07333.pdf))
- CONNECTING IMAGES AND NATURAL LANGUAGE ADISSERTATION(**Andrej Karpathy**)
- Sequence Level Training with Recurrent Neural Networks(ICML2015)
- Deep Feature Flow for Video Recognition(CVPR2017, video object detection, [code.mxnet](https://github.com/msracver/Deep-Feature-Flow), [FlowNet2.pytorch](https://github.com/NVIDIA/flownet2-pytorch))
- Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning(ACL2018, New dataset but not release)

</details>


